# ナイーブベイズ
## 概要
機械学習におけるナイーブベイズアルゴリズムを実装しました。

## リファレンス
### 単純ベイズ(ナイーブベイズ)
- 特徴量の独立性を仮定し、単純に各クラスに分類される確率を計算して分類する機械学習アルゴリズム。
- 確率計算だけで良いので学習コストが非常に低く、ヒューリスティックな場面等での増分学習で活躍します。
- ~~論文における負けモデルとして良く採用されます。~~

#### アルゴリズム
1. ある特徴量のある値におけるクラス所属の**条件付き確率 $P(x_i | c)$**を求めておく。  
  a. 特徴量が離散値の場合： $P(x_i | c) = \frac{|D_{c,x}|+Laplace}{D_c+N_{i}}$
    - 学習データ $D$ 中の相異なるクラス $c$ 数を $N$ とし、 $i$ 番目の特徴量が取り得る相異なる値の数を $N_{i}$ としている
    - このLaplaceによって未知の特徴量が出てきた時に確率が0になる不具合を防ぐテクニックを**ラプラス平滑化**という
  b. 特徴量が連続値の場合： $P(x_i | c) = \frac{1}{\sqrt{2 \pi} \sigma_{c,i}} exp(-\frac{(x_i - \mu_{c,i})^2}{2 \sigma^2_{c,i}})$  
3. 予測したい時はモデル学習時に求めた確率をクラス別にそれぞれ計算し、最も確率の高いクラスに分類する。

<img width="180" alt="スクリーンショット 2023-07-06 22 48 53" src="https://github.com/ARAN1218/ML_Learning/assets/67265109/af80bc12-5567-4f8e-80ae-45d2c183d77d">

