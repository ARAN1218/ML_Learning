# アンサンブル学習アルゴリズム
## 概要
機械学習におけるアンサンブル学習アルゴリズムを実装しました。

## リファレンス
### 決定木(Decision Tree)
- ノンパラメトリックな機械学習アルゴリズムの一種
- やっていることは「データを分割する」ということだけで、実は予測はしていない。
  - 予測はデータを分割した先（葉）に仕込んだ線形回帰等の機械学習アルゴリズムによって行われる。
  - いくつかの機械学習アルゴリズムを組み合わせて使うため、アンサンブル学習の一種として見ることもできる。
  - 同じようなデータに分割した後に学習させるため、これらは非線形を捉えられる。
- データをどのように分けたのかを可視化することにより、非常に解釈性の高いモデルとなっている。
- 決定木は過学習しやすいため、プルーニング（枝刈り）等の手法が提案されている。
- 低バイアス・高バリアンスのモデルであるため、各種アンサンブル学習の基本アルゴリズムとして採用されている。

#### アルゴリズム
1. ある説明変数を選択し、その中から最もデータを分割できる値を探索する。  
(データが分割できた尺度として、標準偏差(回帰)・ジニ不純度(分類)・Information gain(分類)等を計算する。)
3. 1で発見した値でデータの分岐点を作成する。
4. 指定した深さ(max_depth)までノードが伸びた時点で分岐を止め、葉に指定した機械学習アルゴリズムで。
5. 必要に応じてプルーニングを行うために、あまり分割できていない分岐点を探索・置換する。
<img width="262" alt="スクリーンショット 2022-12-13 14 41 39" src="https://user-images.githubusercontent.com/67265109/207239017-7b98e479-559f-410d-b09f-7389b2889448.png">
<img width="285" alt="スクリーンショット 2022-12-13 15 02 44" src="https://user-images.githubusercontent.com/67265109/207238997-db386c2a-afad-4fe2-be98-a1f39bde59fc.png">


### バギング(Bagging)
- 複数の異なる機械学習アルゴリズムを学習させて、それらの予測値の平均を取ることで確率的に汎化誤差を減少させるアルゴリズム。
  - 分類であれば多数決投票やクラスの所属確率、回帰であれば単純な平均値を指す。
- 名称はBootstrap AGGrigatINGから来ている。
- 採用する機械学習アルゴリズムは何でも良く、性能が低いモデル程伸び代がある。

#### アルゴリズム
ある機械学習モデルの残差を求める関数を $f$ と置くと、以下の関係が成り立つ。  
$$Ef^{2}(x) \geq [Ef(x)]^{2}$$
この式から、「複数モデルの予測値の平均のMAEは、複数モデルの予測値のMAEの平均**以下**である」事が分かる。つまり、予測の平均値をとったほうが汎化誤差が小さくなる。そして、誤差がどの程度小さくなるかは「採用したモデル同士の相関関係の低さ」による。  
より異なったモデルの平均の方が汎化誤差が低く、似たようなモデルならあまり変わらない。理想は各モデルの性能が高く、それぞれの相関が低いことである。  
この「相関の低い」モデルを作成するため、このアルゴリズムでは各モデルの学習毎にブートストラップ法によって重複を許した任意の数の学習データを生成・適用する。ブートストラップ法を用いることで統計的ないくつかの異なる機械学習モデルの誤差の平均は、元々のデータに対するモデルの誤差を近似することになるため、バギングはその汎化誤差以下になることが期待できる。  
また、バギングは個々の機械学習モデルの学習は別々に行える（並列処理が可能である）ことから、計算が高速に行える。

<img width="355" alt="スクリーンショット 2022-12-13 23 32 21" src="https://user-images.githubusercontent.com/67265109/207361748-b682c8b4-0ede-484b-8822-1db054c2bc66.png">
<img width="574" alt="スクリーンショット 2022-12-13 23 32 49" src="https://user-images.githubusercontent.com/67265109/207361771-32038e38-8332-46a7-bc23-8d87efb6cb2e.png">


### ランダムフォレスト(Random Forest)
- バギングの「学習データ数が多いと、結局似たようなモデルが出来上がってしまい、各モデルの相関が高くなってしまう」という弱点を克服したバギングの応用アルゴリズム。
- 現在も予測性能の高い機械学習アルゴリズムとして知られており、特徴量重要度として幅広く活用

#### アルゴリズム
ランダムフォレストは、学習データをバギングによってランダムに生成・適用するのみならず、使用する「特徴量の次元数」をもランダムで決定することで、より相関の低い決定木モデルを生成できるという理屈のもと作られたアルゴリズムである。精度が向上する理由についてはバギングの説明と同様である。機械学習モデルに決定木を採用した理由は、決定木が低バイアス・高バリアンスのモデルだからである。  
ランダムフォレストは採用されなかった特徴量**特徴量重要度**というものを計算できる。ブートストラップによって選ばれない学習データは統計的に全体の約36%あり、そのデータをテストデータとして[PFI](https://github.com/ARAN1218/ML_Learning/tree/main/XAI#pfipermutation-feature-importance)を計算することで求める。また、単に各決定木のジニ不純度の減少率を平均したものも特徴量重要度として計算される。  
更にバギングは並列処理が可能であることから、それなりに高速・高精度の機械学習アルゴリズムとして運用できる。
<img width="578" alt="スクリーンショット 2022-12-14 1 47 09" src="https://user-images.githubusercontent.com/67265109/207393610-4b415e0d-4331-4ceb-89a9-75acb6d53f94.png">


### アダブースティング(AdaBoosting)
- アンサンブル学習の内、ブースティング(Boosting)と呼ばれるメタアルゴリズムの基本的なアルゴリズム。
- 一般的にバギングよりも予測性能が高くなりやすいとされている。
- モデルの学習を並列で行えないため、学習時間が長い。
- 正解ラベルの貼り間違えや異常値に弱く、過学習しやすいという欠点を持つ。
- オリジナルのアダブーストは二値分類専用アルゴリズムであるが、それらを多クラス分類タスクや回帰タスクに拡張した改良版アルゴリズムが存在する。

#### アルゴリズム(オリジナルAdaBoost)
1. 使用する機械学習モデルを選択し、学習データにつける重みベクトル・各モデルにつける貢献値を初期化する。
2. 重みを加えたデータで機械学習モデルを学習させる。
3. 学習によって予測しにくいデータに重みを移動させるように重みを更新する。
4. 予測が不正解の位置にある重みの合計を求め、それを元にそのモデルの貢献度を計算・保存する。
5. 貢献度に応じて重みを調整・正規化し、誤差がある程度小さくなるまでステップ2から繰り返す。

<img width="322" alt="original" src="https://user-images.githubusercontent.com/67265109/208430921-be7f1ae5-2746-4f62-9a50-3a68ecadfcb4.png">
<img width="327" alt="M1" src="https://user-images.githubusercontent.com/67265109/208430938-25486eb4-f604-4811-bcc4-299dd80b7312.png">
<img width="342" alt="RT" src="https://user-images.githubusercontent.com/67265109/208430955-574da0d3-2863-430c-a8a4-5ca82c70d1a3.png">
<img width="335" alt="R2" src="https://user-images.githubusercontent.com/67265109/208430962-41de7c2f-75b6-476c-b599-17125bf9988b.png">




### XGBoost


### LightGBM


### Catboost


### スタッキング(Stacking)


### ブレンディング(Blending)


### バンピング(Bumping)


### バギングをブースティングしてスタッキングする（？）


## 参考文献
- 作ってわかる! アンサンブル学習アルゴリズム入門、坂本俊之、C&R研究所(2019)

![Unknown](https://user-images.githubusercontent.com/67265109/207372585-25c2cedf-cfc0-40f9-876c-74790433cb08.jpeg)
