# アンサンブル学習アルゴリズム
## 概要
機械学習におけるアンサンブル学習アルゴリズムを実装しました。

## リファレンス
### 決定木(Decision Tree)
- ノンパラメトリックな機械学習アルゴリズムの一種
- やっていることは「データを分割する」ということだけで、実は予測はしていない。
  - 予測はデータを分割した先（葉）に仕込んだ線形回帰等の機械学習アルゴリズムによって行われる。
  - いくつかの機械学習アルゴリズムを組み合わせて使うため、アンサンブル学習の一種として見ることもできる。
  - 同じようなデータに分割した後に学習させるため、これらは非線形を捉えられる。
- データをどのように分けたのかを可視化することにより、非常に解釈性の高いモデルとなっている。
- 決定木は過学習しやすいため、プルーニング（枝刈り）等の手法が提案されている。
- 低バイアス・高バリアンスのモデルであるため、各種アンサンブル学習の基本アルゴリズムとして採用されている。

#### アルゴリズム
1. ある説明変数を選択し、その中から最もデータを分割できる値を探索する。  
(データが分割できた尺度として、標準偏差(回帰)・ジニ不純度(分類)・Information gain(分類)等を計算する。)
3. 1で発見した値でデータの分岐点を作成する。
4. 指定した深さ(max_depth)までノードが伸びた時点で分岐を止め、葉に指定した機械学習アルゴリズムで。
5. 必要に応じてプルーニングを行うために、あまり分割できていない分岐点を探索・置換する。
<img width="262" alt="スクリーンショット 2022-12-13 14 41 39" src="https://user-images.githubusercontent.com/67265109/207239017-7b98e479-559f-410d-b09f-7389b2889448.png">
<img width="285" alt="スクリーンショット 2022-12-13 15 02 44" src="https://user-images.githubusercontent.com/67265109/207238997-db386c2a-afad-4fe2-be98-a1f39bde59fc.png">


### バギング(Bagging)
- 複数の異なる機械学習アルゴリズムを学習させて、それらの予測値の平均を取ることで確率的に汎化誤差を減少させるアルゴリズム。
  - 分類であれば多数決投票やクラスの所属確率、回帰であれば単純な平均値を指す。
- 名称はBootstrap AGGrigatINGから来ている。
- 採用する機械学習アルゴリズムは何でも良く、性能が低いモデル程伸び代がある。

#### アルゴリズム
ある機械学習モデルの残差を求める関数を $f$ と置くと、以下の関係が成り立つ。  
$$Ef^{2}(x) \geq [Ef(x)]^{2}$$
この式から、「複数モデルの予測値の平均のMAEは、複数モデルの予測値のMAEの平均**以下**である」事が分かる。つまり、予測の平均値をとったほうが汎化誤差が小さくなる。そして、誤差がどの程度小さくなるかは「採用したモデル同士の相関関係の低さ」による。  
より異なったモデルの平均の方が汎化誤差が低く、似たようなモデルならあまり変わらない。理想は各モデルの性能が高く、それぞれの相関が低いことである。  
この「相関の低い」モデルを作成するため、このアルゴリズムでは各モデルの学習毎にブートストラップ法によって重複を許した任意の数の特徴量を生成・適用する。ブートストラップ法を用いることで統計的ないくつかの異なる機械学習モデルの誤差の平均は、元々のデータに対するモデルの誤差を近似することになるため、バギングはその汎化誤差以下になることが期待できる。  
また、バギングは個々の機械学習モデルの学習は別々に行える（並列処理が可能である）ことから、計算が高速に行える。

<img width="355" alt="スクリーンショット 2022-12-13 23 32 21" src="https://user-images.githubusercontent.com/67265109/207361748-b682c8b4-0ede-484b-8822-1db054c2bc66.png">
<img width="574" alt="スクリーンショット 2022-12-13 23 32 49" src="https://user-images.githubusercontent.com/67265109/207361771-32038e38-8332-46a7-bc23-8d87efb6cb2e.png">


### ランダムフォレスト(Random Forest)
- バギングの「学習データ数が多いと、結局似たようなモデルが出来上がってしまい、各モデルの相関が高くなってしまう」という弱点を克服したバギングの応用アルゴリズム。
- 学習データをバギングによってランダムに生成・適用するのみならず、使用する「特徴量の次元数」をもランダムで決定することで、より相関の低い決定木モデルを生成できるという理屈のもと作られたアルゴリズムである。
- 現在も予測性能の高い機械学習アルゴリズムとして知られている。
- 機械学習モデルに決定木を採用した理由は、決定木が低バイアス・高バリアンスのモデルだからである。
- バギングは並列処理が可能であることから、それなりに高速・高精度の機械学習アルゴリズムとして運用できる。
- ランダムフォレストは採用されなかったデータを用いて**特徴量重要度**というものを計算できる。ブートストラップによって選ばれない学習データは統計的に全体の約36%あり、そのデータをテストデータとして[PFI](https://github.com/ARAN1218/ML_Learning/tree/main/XAI#pfipermutation-feature-importance)を計算することで求める。また、単に各決定木のジニ不純度の減少率を平均したものも特徴量重要度として計算される。

#### アルゴリズム
1. 決定木モデルを用意し、初期化する。
2. ある決定木モデルに対して、全特徴量の中からある次元だけランダムに抽出し、それだけを学習に用いると決める。
3. 2で決めた次元の特徴量から、ブートストラップ法によってランダムに重複を許した任意の数の特徴量を生成・適用する。
4. 2,3の手順を指定の本数の決定木モデルに対して行う。
5. 予測の際は全モデルの平均値を最終的な出力とする。

<img width="578" alt="スクリーンショット 2022-12-14 1 47 09" src="https://user-images.githubusercontent.com/67265109/207393610-4b415e0d-4331-4ceb-89a9-75acb6d53f94.png">


### アダブースティング(AdaBoosting)
- アンサンブル学習の内、ブースティング(Boosting)と呼ばれるメタアルゴリズムの基本的なアルゴリズム。
- 一般的にバギングよりも予測性能が高くなりやすいとされている。
- モデルの学習を並列で行えないため、学習時間が長い。
- 正解ラベルの貼り間違えや異常値に弱く、過学習しやすいという欠点を持つ。
- オリジナルのアダブーストは二値分類専用アルゴリズムであるが、それらを多クラス分類タスクや回帰タスクに拡張した改良版アルゴリズムが存在する。

#### アルゴリズム(オリジナルAdaBoost)
1. 使用する機械学習モデルを選択し、学習データにつける重みベクトル・各モデルにつける貢献値を初期化する。
2. 重みを加えたデータで機械学習モデルを学習させる。
3. 学習によって予測しにくいデータに重みを移動させるように重みを更新する。
4. 予測が不正解の位置にある重みの合計を求め、それを元にそのモデルの貢献度を計算・保存する。
5. 貢献度に応じて重みを調整・正規化し、誤差がある程度小さくなるまでステップ2から繰り返す。

<img width="322" alt="original" src="https://user-images.githubusercontent.com/67265109/208430921-be7f1ae5-2746-4f62-9a50-3a68ecadfcb4.png">

#### アルゴリズム(M1)
1. 使用する機械学習モデルを選択し、学習データにつける重みベクトル・各モデルにつける貢献値を初期化する。
2. 重みを加えたデータで機械学習モデルを学習させる。
3. その機械学習モデルの出力から $\frac{正解したデータの重み合計}{不正解したデータの重み合計}$ で定義される0~1の値を持つ $\beta$ を求め、これを先程の予測で正解したデータの重みにかけて小さくする。
4. ブースティングの条件が終わるまで2,3の手順を繰り返す。(終了条件：学習の繰り返し回数が終わる、損失が0になる等)
5. 予測の際は学習した機械学習モデルを予測データを用いて予測させ、その出力地に基づいて分類したクラスに $\log \frac{1}{\beta}$ で求められる各モデルの「貢献値」を足していき、最も貢献値の高いクラスを最終的な予測値とする。

<img width="327" alt="M1" src="https://user-images.githubusercontent.com/67265109/208430938-25486eb4-f604-4811-bcc4-299dd80b7312.png">

#### アルゴリズム(RT)
1. 使用する機械学習モデルを選択し、学習データにつける重みベクトル・各モデルにつける貢献値を初期化する。
2. 重みを加えたデータで機械学習モデルを学習させる。
3. その機械学習モデルの出力の残差の絶対値が予め定めた閾値よりも小さい場合に「正解」、大きい場合に「不正解」として扱い、正解のデータの重みに対して $\frac{正解したデータの重み合計}{不正解したデータの重み合計}$ で定義される0~1の値を持つ $\beta$ の二乗をかけ、重みを小さくする。(二乗である理由は、二乗誤差を最小化するため)
4. 予め定めた回数分、2,3の手順を繰り返す。
5. 予測の際は、各モデルの重み付き平均 $\frac{\sum_{t} \log \frac{1}{\beta_t} ・ f_t(x)}{\sum_{t} \log \frac{1}{\beta_t}}$ となる。

<img width="342" alt="RT" src="https://user-images.githubusercontent.com/67265109/208430955-574da0d3-2863-430c-a8a4-5ca82c70d1a3.png">

#### アルゴリズム(R2)
1. 使用する機械学習モデルを選択し、学習データにつける重みベクトル・各モデルにつける貢献値を初期化する。
2. 重みを加えたデータで機械学習モデルを学習させる。
3. その機械学習モデルの出力の残差の絶対値を、それらの値の最大値で割って正規化したものを各データの重みにかけて小さくする。(こうすることで残差の大きい=判断のしにくいデータの重みは大きく、そうでない重みは小さくできる)
4. 損失の値が $\frac{1}{2}$ より大きくなるまで、2,3の手順を繰り返す。
5. 予測の際は、全ての機械学習モデルについて、モデルの出力の値が小さい順番でその貢献値 $\log \frac{1}{\beta}$ を足していき、全貢献値の半分の位置にあるモデルの出力を最終的な出力にする。(こうすることで、重みつき平均よりも対象の「トレンド」にフィットしたモデルを丁度よく選択できると考えられるため)

<img width="335" alt="R2" src="https://user-images.githubusercontent.com/67265109/208430962-41de7c2f-75b6-476c-b599-17125bf9988b.png">



### モデル選択法
- ある複数のモデルから性能の良いモデルをピックアップして使用するアンサンブル学習の一種
- ※今回作成したプログラムでは全てのモデルで同一のデータを用いて学習させているが、実際は学習に用いるデータでモデルを差別化することが考えられる。

#### 交差検証(Cross Validation Select)
- データをK個に分割し、その内の1つをテストデータに設定して予測精度評価をK回行うモデル評価方法であるクロスバリデーション(KFold法)を用いてモデルを評価し、最も良いモデルを選択する手法。

##### アルゴリズム
1. 複数の学習器を用意する。
2. それらを個別にクロスバリデーションを用いて予測精度を評価し、最も精度の良いモデルを一つ選択する。


#### 情報量基準による選択法(Information Criterion Select)
- 情報量基準という評価指標を用いてモデルを評価し、最も性能が良いモデルを選択する手法。
- メジャーな情報量基準として、赤池情報量基準(AIC)とベイズ情報量基準(BIC)がある。
  - AIC: モデルの予測性能に加えて、モデルの複雑さを考慮した評価指標( $- 2\log{L} + 2k$ )
  - BIC: モデルの予測性能に加えて、AICよりもシンプルなモデルをより高く評価する評価指標( $- 2\log{L} + k\log{n}$ )
  - $k：パラメータ数(説明変数の種類数)、L：尤度関数、n：サンプルサイズ$
- モデル選択を**モデル自体の評価**を用いて定量的に行うことができる手法となっている。
- 参考：http://www.radio3.ee.uec.ac.jp/ronbun/TR_YK_048_AIC.pdf

##### アルゴリズム
1. 複数の学習器を用意し、評価に用いる情報量基準の種類を決定する。
2. それらを個別に情報量基準を用いて評価し、最も精度の良いモデルを一つ選択する。


#### バンピング(Bumping)
- バギングと同様にして学習器を複数作り、各学習器で訓練データの予測精度を調べて最も精度の良いモデルを一つだけ選択する手法。
- 一見にして意味がないように思えるが、ブートストラッピングによって予測にとって害悪なデータを排除したモデルを生成する可能性があり、そのモデルは比較的性能が良くなるはずなのでメリットがある。

##### アルゴリズム
1. バギングを用いて学習器を複数個作成する。
2. それらを個別にクロスバリデーションを用いて評価し、最も精度の良いモデルを一つ選択する。
3. 予測の際は、2で選択したモデル単体での予測となる。


### モデル平均法
- ある複数のモデルの出力をまとめることで性能の向上を図るアンサンブル学習の一種

#### ゲーティング(Gating)
- 全てのモデルに対して重みを与え、それらの出力を合算するアルゴリズム。
  - 最後の合算は単純パーセプトロンのそれになる。

##### アルゴリズム
1. 各層に用いる機械学習モデルを選定する。
2. 最初の層(level0)のモデルを学習させる。
3. 2で学習させたモデルを学習データで予測させ、その出力を次の層(level1)の単純パーセプトロンの入力にして学習させる。
4. 予測の際も2,3と同様の流れで出力を得る。


#### スタッキング(Stacking)
- ある機械学習モデルの出力を下層の機械学習モデルの入力とし、出力結果をまとめるアンサンブル学習アルゴリズムの一種。
- ゲーティングとは違い、最終層の機械学習モデルを自由に選ぶことができる。
- 2層構成のスタッキングをブレンディング(Blending)と呼ぶことがある。
  - [狭義]スタッキングは評価にKFoldを用いるのに対して、それをホールドアウトでやる方法。実装がシンプルになるが過学習気味になるという特徴がある。
  - 参考：https://atmarkit.itmedia.co.jp/ait/articles/2112/02/news016.html
- バギングやブースティングと比較して、あまりメジャーなアンサンブル学習アルゴリズムではないが、Kaggle等では広く活用されている。

##### アルゴリズム
1. 各層に用いる機械学習モデルを選定する。
2. 最初の層(level0)のモデルを学習させる。
3. 2で学習させたモデルを学習データで予測させ、その出力を次の層(level1)のモデルの入力にして学習させる。
4. 指定した層まで3を繰り返す。
5. 予測の際も2,3と同様の流れで出力を得る。

<img width="571" alt="スクリーンショット 2023-01-29 18 14 27" src="https://user-images.githubusercontent.com/67265109/215316774-6521ac7a-76eb-4653-8767-2f9ae822942b.png">


#### NFold平均(NFoldMean)
- データをK分割してK個のモデルを作り、それらの出力を平均するアルゴリズム。
- バギングとは違い、重複なしの部分集合データを学習データとして採用している。
- バギングはある程度個々のモデルが不完全であることを想定しているが、NFold平均ではその前提が薄れている。つまり、予め精度を最大化するように設計されたアンサンブル学習アルゴリズムに対してはNFold平均の方が良好に動作する可能性がある。

##### アルゴリズム
1. データをK分割する。
2. K分割したデータで同一の機械学習アルゴリズムをK個学習させる。
3. 予測の際はK個のモデルの出力の平均・多数決等で集約した結果を採用する。


#### 情報量基準による平均法(Smoothed-ICMean)
- 各モデルの情報量基準の値によって、出力の重み付け平均を取るアルゴリズム。
- 情報量基準による選択法とは違い、モデルを一つに絞るのではなく全てのモデルの出力を活用する。
- 情報量基準については[情報量基準による選択法](https://github.com/ARAN1218/ML_Learning/edit/main/Ensemble/README.md#情報量基準による選択法information-criterion-select)で説明した。

##### アルゴリズム
1. 複数の学習器を用意し、評価に用いる情報量基準の種類を決定する。
2. それらを個別に情報量基準を用いて評価する。
3. 2の結果に基づき、各モデルの出力の重み付け平均をとったものを最終的な出力とする。


### バギングをブースティングしてスタッキングする（？）


### XGBoost


### LightGBM


### Catboost




## 参考文献
- 作ってわかる! アンサンブル学習アルゴリズム入門、坂本俊之、C&R研究所(2019)
- Bagging predictors、Breiman, Leo、Machine learning(1996)
- Random forests、Breiman, Leo、Machine learning(2001)
- A decision-theoretic generalization of on-line learning and an application to boosting、Freund, Yoav、Journal of computer and system sciences(1997)
- Greedy function approximation: a gradient boosting machine、Friedman, Jerome H、Annals of statistics(2001)
- Xgboost: A scalable tree boosting system、Chen, Tianqi、Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining(2016)
- Lightgbm: A highly efficient gradient boosting decision tree、Ke, Guolin、Advances in neural information processing systems(2017)
- CatBoost: unbiased boosting with categorical features、Prokhorenkova, Liudmila、Advances in neural information processing systems(2018)

![Unknown](https://user-images.githubusercontent.com/67265109/207372585-25c2cedf-cfc0-40f9-876c-74790433cb08.jpeg)

